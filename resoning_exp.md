# 实验：对比不同感知与对齐方式的MLLM的推理能力

## 实验目标

本实验旨在比较不同感知机制与模态对齐方式的多模态大语言模型在多模态推理任务上的表现差异，探索感知与对齐方式是否对模型推理能力产生实质影响。

## 实验变量设置

对于三种典型的感知与对齐方式，我们分别选取代表性的模型作为实验对象。

| 模型组别 | 感知方式            | 对齐方式                        | 示例模型          |
| -------- | ------------------- | ------------------------------- | ----------------- |
| A        | CLIP 图像整体特征   | Adapter（线性映射）             | LLaVA             |
| B        | CLIP Patch 特征     | Q-Former（主动提问 + 语义压缩） | BLIP-2            |
| C        | ViT Patch Embedding | 拼接后输入 LLM                  | MiniGPT-4（早期） |

## 问题设计

我们设计了3类共15-20道推理问题，涵盖了不同类型的多模态推理。

- **常识推理类**：需要结合图像和背景知识判断
- **空间/关系推理类**：判断位置、方向、比较等关系
- **多步因果类**：图+文描述下，推断原因或未来结果

## 输入与输出设计

**统一的输入格式如下**：

- 图像
- 问题（中文/英文）
- 无prompt/CoT提示

**同时，要记录模型的输出**：

- 回答的是否正确
- 是否有引用视觉信息
- 若输入使用了CoT提示，则记录其CoT步骤

## 评估指标

| 维度           | 指标                             | 说明                                 |
| -------------- | -------------------------------- | ------------------------------------ |
| 正确性         | QA Accuracy                      | 闭集题判断是否选对                   |
| 推理质量       | GPT-4 自动评分 / 人工评分（1–5） | 语言合理性、是否有逻辑、是否结合图像 |
| 推理步骤完整性 | CoT 步骤结构与语义一致性         | 仅在 CoT 模式下打分                  |
| 多模态引用程度 | 是否提及图像关键信息             | 标记“强相关 / 弱相关 / 无引用”       |

